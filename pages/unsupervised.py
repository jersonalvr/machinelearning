# pages/unsupervised.py
import streamlit as st
import pandas as pd
import numpy as np
import plotly.express as px
import plotly.graph_objects as go
from sklearn.preprocessing import StandardScaler
from sklearn.decomposition import PCA
from sklearn.cluster import KMeans, DBSCAN, AgglomerativeClustering
from sklearn.manifold import TSNE
from sklearn.metrics import silhouette_score, calinski_harabasz_score, davies_bouldin_score
import google.generativeai as genai
import umap

def show_unsupervised():
    # Verificar datos preparados
    if 'prepared_data' not in st.session_state or st.session_state.prepared_data is None:
        st.warning("Por favor, carga y prepara tus datos primero en la pÃ¡gina de PreparaciÃ³n.")
        return

    # Obtener datos
    data = st.session_state.prepared_data

    # Seleccionar columnas numÃ©ricas
    numeric_cols = data.select_dtypes(include=['int64', 'float64']).columns.tolist()

    if not numeric_cols:
        st.error("No hay variables numÃ©ricas para realizar anÃ¡lisis no supervisado.")
        return
    
    # SelecciÃ³n de caracterÃ­sticas
    feature_cols = st.multiselect(
        "Seleccionar Variables para AnÃ¡lisis",
        numeric_cols,
        default=numeric_cols[:min(5, len(numeric_cols))]
    )

    if not feature_cols:
        st.warning("Selecciona al menos una variable.")
        return
    
    # SelecciÃ³n de mÃ©todo con selecciÃ³n mÃºltiple
    metodos = st.multiselect(
        "Seleccionar MÃ©todos",
        [
            "K-Means",
            "DBSCAN",
            "Clustering JerÃ¡rquico",
            "AnÃ¡lisis de Componentes Principales (PCA)",
            "t-SNE",
            "UMAP"
        ],
        key='metodos'
    )

    # Variables para almacenar mÃ©tricas globales
    global_silhouette = None
    global_calinski = None
    global_davies = None
    method_details = {}

    # Calcular nÃºmero de columnas dinÃ¡micamente
    if metodos:
        # Crear columnas con mÃ©todos seleccionados
        cols = st.columns(len(metodos))
        
        # Iterar sobre mÃ©todos seleccionados
        for i, metodo in enumerate(metodos):
            with cols[i]:
                # Contenedor para cada mÃ©todo
                with st.container():
                    st.subheader(f"Resultados de {metodo}")

                    # Configuraciones especÃ­ficas por mÃ©todo
                    n_clusters = None
                    init_method = None
                    eps = None
                    min_samples = None
                    n_components = None

                    # ParÃ¡metros especÃ­ficos por mÃ©todo
                    if metodo in ["K-Means", "Clustering JerÃ¡rquico"]:
                        n_clusters = st.slider(f"NÃºmero de Clusters ({metodo})", 2, 10, 3, key=f'n_clusters_{i}')

                    if metodo == "K-Means":
                        init_method = st.selectbox(
                            "MÃ©todo de InicializaciÃ³n", 
                            ["k-means++", "random"], 
                            key=f'init_method_{i}'
                        )

                    if metodo == "DBSCAN":
                        eps = st.slider(f"Epsilon ({metodo})", 0.1, 2.0, 0.5, key=f'eps_{i}')
                        min_samples = st.slider(f"MÃ­nimo de Muestras ({metodo})", 2, 20, 5, key=f'min_samples_{i}')

                    if metodo in ["t-SNE", "UMAP"]:
                        n_components = st.slider(f"NÃºmero de Componentes ({metodo})", 2, 3, 2, key=f'n_components_{i}')

                    # Preparar datos
                    X = data[feature_cols]
                    scaler = StandardScaler()
                    X_scaled = scaler.fit_transform(X)

                    # Construir texto de parÃ¡metros
                    params_text = "Los parÃ¡metros utilizados en el mÃ©todo son:\n"

                    if metodo == "K-Means":
                        params_text += f"- NÃºmero de Clusters: {n_clusters}\n"
                        params_text += f"- MÃ©todo de InicializaciÃ³n: {init_method}\n"
                    elif metodo == "DBSCAN":
                        params_text += f"- Epsilon (eps): {eps}\n"
                        params_text += f"- MÃ­nimo de Muestras por Cluster: {min_samples}\n"
                    elif metodo == "Clustering JerÃ¡rquico":
                        params_text += f"- NÃºmero de Clusters: {n_clusters}\n"
                    elif metodo == "AnÃ¡lisis de Componentes Principales (PCA)":
                        params_text += "No se especificaron parÃ¡metros adicionales.\n"
                    elif metodo == "t-SNE":
                        params_text += f"- NÃºmero de Componentes: {n_components}\n"
                    elif metodo == "UMAP":
                        params_text += f"- NÃºmero de Componentes: {n_components}\n"
                    else:
                        params_text += "No se especificaron parÃ¡metros adicionales.\n"

                    # Procesamiento segÃºn mÃ©todo
                    try:
                        if metodo == "K-Means":
                            modelo = KMeans(
                                n_clusters=n_clusters,
                                random_state=42,
                                n_init=10,
                                init=init_method
                            )
                            clusters = modelo.fit_predict(X_scaled)
                            
                            # MÃ©tricas de clustering
                            silhouette = silhouette_score(X_scaled, clusters)
                            calinski = calinski_harabasz_score(X_scaled, clusters)
                            davies = davies_bouldin_score(X_scaled, clusters)

                            # Almacenar mÃ©tricas globales
                            global_silhouette = silhouette
                            global_calinski = calinski
                            global_davies = davies

                            st.subheader("MÃ©tricas de Clustering")
                            col1, col2, col3 = st.columns(3)
                            with col1:
                                st.metric("Silhouette Score", f"{silhouette:.4f}")
                            with col2:
                                st.metric("Calinski-Harabasz", f"{calinski:.2f}")
                            with col3:
                                st.metric("Davies-Bouldin", f"{davies:.4f}")

                            # VisualizaciÃ³n de clusters
                            pca = PCA(n_components=2)
                            X_pca = pca.fit_transform(X_scaled)
                            fig = px.scatter(
                                x=X_pca[:, 0],
                                y=X_pca[:, 1],
                                color=clusters.astype(str),
                                title=f"Clusters {metodo}",
                                labels={'x': 'PCA 1', 'y': 'PCA 2'}
                            )
                            st.plotly_chart(fig)

                            with st.expander("ðŸ§  InterpretaciÃ³n de K-Means"):
                                st.markdown("""
                                    ### Objetivo: Dividir los datos en K grupos distintos

                                    **CaracterÃ­sticas:**
                                    - Cada punto pertenece al cluster con centro mÃ¡s cercano
                                    - Minimiza la varianza dentro de cada cluster
                                    - Los centroides representan el centro de cada grupo

                                    **MÃ©tricas Clave:**
                                    - **Silhouette Score:** Mide quÃ© tan similar es un punto a su propio cluster comparado con otros clusters
                                        - Rango: -1 a 1 (mayor es mejor)
                                        - > 0.7: Estructura fuerte
                                        - 0.5-0.7: Estructura razonable
                                        - < 0.5: Estructura dÃ©bil
                                    - **Calinski-Harabasz:** EvalÃºa la separaciÃ³n entre clusters
                                        - Mayor valor indica mejor definiciÃ³n de clusters
                                    - **Davies-Bouldin:** Mide la similitud promedio entre cada cluster
                                        - Menor valor indica mejor separaciÃ³n entre clusters

                                    **InterpretaciÃ³n PrÃ¡ctica:**
                                    - **NÃºmero de Clusters (K):** Define cuÃ¡ntos grupos queremos encontrar
                                    - **InicializaciÃ³n:** Afecta la posiciÃ³n inicial de los centroides
                                    - Ãštil para segmentaciÃ³n de clientes, agrupaciÃ³n de productos, etc.
                                    """)

                        elif metodo == "DBSCAN":
                            modelo = DBSCAN(eps=eps, min_samples=min_samples)
                            clusters = modelo.fit_predict(X_scaled)
                            
                            # Count unique clusters (excluding noise points)
                            unique_clusters = np.setdiff1d(np.unique(clusters), [-1])
                            
                            if len(unique_clusters) > 1:
                                # Metrics for non-noise clusters
                                non_noise_mask = clusters != -1
                                X_scaled_clustered = X_scaled[non_noise_mask]
                                clusters_clustered = clusters[non_noise_mask]
                                
                                # Calculate metrics
                                silhouette = silhouette_score(X_scaled_clustered, clusters_clustered)
                                calinski = calinski_harabasz_score(X_scaled_clustered, clusters_clustered)
                                davies = davies_bouldin_score(X_scaled_clustered, clusters_clustered)

                                st.subheader("MÃ©tricas de Clustering")
                                col1, col2, col3 = st.columns(3)
                                with col1:
                                    st.metric("Silhouette Score", f"{silhouette:.4f}")
                                with col2:
                                    st.metric("Calinski-Harabasz", f"{calinski:.2f}")
                                with col3:
                                    st.metric("Davies-Bouldin", f"{davies:.4f}")

                                # Visualization using PCA
                                pca = PCA(n_components=2)
                                X_pca = pca.fit_transform(X_scaled)
                                
                                # Create color mapping including noise points
                                color_map = clusters.copy().astype(str)
                                color_map[clusters == -1] = 'Noise'
                                
                                fig = px.scatter(
                                    x=X_pca[:, 0],
                                    y=X_pca[:, 1],
                                    color=color_map,
                                    title=f"Clusters {metodo}",
                                    labels={'x': 'PCA 1', 'y': 'PCA 2'}
                                )
                                st.plotly_chart(fig)
                                
                                # Additional information about clustering
                                st.write(f"Total Clusters Found: {len(unique_clusters)}")
                                st.write(f"Noise Points: {np.sum(clusters == -1)}")
                            else:
                                st.warning(f"DBSCAN no pudo encontrar mÃºltiples clusters con los parÃ¡metros actuales. Intente ajustar eps ({eps}) o min_samples ({min_samples}).")
                            with st.expander("ðŸ§  InterpretaciÃ³n de DBSCAN"):
                                st.markdown("""
                                    ### Objetivo: Encontrar clusters basados en densidad

                                    **CaracterÃ­sticas:**
                                    - Puede detectar clusters de formas arbitrarias
                                    - Identifica puntos de ruido (outliers)
                                    - No requiere especificar nÃºmero de clusters

                                    **ParÃ¡metros Clave:**
                                    - **Epsilon (eps):** Radio mÃ¡ximo entre puntos del mismo cluster
                                        - Si es muy pequeÃ±o: Muchos clusters pequeÃ±os/ruido
                                        - Si es muy grande: Pocos clusters grandes
                                    - **Min Samples:** NÃºmero mÃ­nimo de puntos para formar un cluster
                                        - Afecta la sensibilidad a ruido
                                        - Valores tÃ­picos: 2 * n_features a 2 * n_features + 1

                                    **InterpretaciÃ³n PrÃ¡ctica:**
                                    - Ideal para datos con ruido
                                    - Bueno para detectar clusters de formas irregulares
                                    - Los puntos marcados como ruido (-1) son potenciales outliers
                                    """)
                        elif metodo == "Clustering JerÃ¡rquico":
                            modelo = AgglomerativeClustering(n_clusters=n_clusters)
                            clusters = modelo.fit_predict(X_scaled)
                            
                            # MÃ©tricas de clustering
                            silhouette = silhouette_score(X_scaled, clusters)
                            calinski = calinski_harabasz_score(X_scaled, clusters)
                            davies = davies_bouldin_score(X_scaled, clusters)

                            # Almacenar mÃ©tricas globales
                            global_silhouette = silhouette
                            global_calinski = calinski
                            global_davies = davies

                            st.subheader("MÃ©tricas de Clustering")
                            col1, col2, col3 = st.columns(3)
                            with col1:
                                st.metric("Silhouette Score", f"{silhouette:.4f}")
                            with col2:
                                st.metric("Calinski-Harabasz", f"{calinski:.2f}")
                            with col3:
                                st.metric("Davies-Bouldin", f"{davies:.4f}")

                            # VisualizaciÃ³n de clusters
                            pca = PCA(n_components=2)
                            X_pca = pca.fit_transform(X_scaled)
                            fig = px.scatter(
                                x=X_pca[:, 0],
                                y=X_pca[:, 1],
                                color=clusters.astype(str),
                                title=f"Clusters {metodo}",
                                labels={'x': 'PCA 1', 'y': 'PCA 2'}
                            )
                            st.plotly_chart(fig)
                            with st.expander("ðŸ§  InterpretaciÃ³n de Clustering JerÃ¡rquico"):
                                st.markdown("""
                                    ### Objetivo: Crear una jerarquÃ­a de clusters

                                    **CaracterÃ­sticas:**
                                    - Construye una jerarquÃ­a de clusters (dendrograma)
                                    - Puede ser aglomerativo (bottom-up) o divisivo (top-down)
                                    - No requiere especificar nÃºmero de clusters inicialmente

                                    **MÃ©tricas Clave:**
                                    - **Silhouette Score:** EvalÃºa la calidad de los clusters
                                    - **Calinski-Harabasz:** Mide la separaciÃ³n entre clusters
                                    - **Davies-Bouldin:** EvalÃºa la similaridad entre clusters

                                    **InterpretaciÃ³n PrÃ¡ctica:**
                                    - Ãštil cuando se busca una estructura jerÃ¡rquica
                                    - Permite visualizar relaciones entre grupos
                                    - Ayuda a determinar el nÃºmero Ã³ptimo de clusters
                                    - Ideal para taxonomÃ­as y estructuras organizativas
                                    """)
                        elif metodo == "AnÃ¡lisis de Componentes Principales (PCA)":
                            pca = PCA()
                            X_pca = pca.fit_transform(X_scaled)

                            # Varianza explicada
                            varianza_explicada = pca.explained_variance_ratio_

                            # Tabla de varianza explicada
                            varianza_df = pd.DataFrame({
                                'Componente': range(1, len(varianza_explicada) + 1),
                                'Varianza Explicada (%)': varianza_explicada * 100,
                                'Varianza Acumulada (%)': np.cumsum(varianza_explicada) * 100
                            })
                            st.dataframe(varianza_df)

                            # GrÃ¡fico de varianza explicada
                            fig_varianza = px.line(
                                varianza_df,
                                x='Componente',
                                y='Varianza Acumulada (%)',
                                title='Varianza Explicada Acumulada'
                            )
                            st.plotly_chart(fig_varianza)
                            with st.expander("ðŸ§  InterpretaciÃ³n de AnÃ¡lisis de Componentes Principales (PCA)"):
                               st.markdown("""
                                    ### Objetivo: Reducir dimensionalidad preservando varianza

                                    **CaracterÃ­sticas:**
                                    - Transforma variables correlacionadas en componentes no correlacionados
                                    - Mantiene la mayor cantidad de informaciÃ³n posible
                                    - Ordena componentes por importancia

                                    **InterpretaciÃ³n de Resultados:**
                                    - **Varianza Explicada:** Porcentaje de informaciÃ³n retenida
                                        - Suma acumulada ayuda a decidir nÃºmero de componentes
                                        - Se busca tÃ­picamente 80-90% de varianza acumulada
                                    - **Componentes Principales:**
                                        - Primer componente: DirecciÃ³n de mÃ¡xima varianza
                                        - Componentes subsecuentes: Ortogonales entre sÃ­

                                    **Aplicaciones PrÃ¡cticas:**
                                    - ReducciÃ³n de dimensionalidad
                                    - VisualizaciÃ³n de datos multidimensionales
                                    - EliminaciÃ³n de multicolinealidad
                                    - CompresiÃ³n de datos
                                    """) 
                        elif metodo == "t-SNE":
                            tsne = TSNE(n_components=n_components, random_state=42)
                            X_tsne = tsne.fit_transform(X_scaled)

                            if n_components == 2:
                                fig_tsne = px.scatter(
                                    x=X_tsne[:, 0],
                                    y=X_tsne[:, 1],
                                    title='VisualizaciÃ³n t-SNE',
                                    labels={'x': 't-SNE 1', 'y': 't-SNE 2'}
                                )
                                st.plotly_chart(fig_tsne)
                            else:
                                fig_tsne = go.Figure(data=[
                                    go.Scatter3d(
                                        x=X_tsne[:, 0],
                                        y=X_tsne[:, 1],
                                        z=X_tsne[:, 2],
                                        mode='markers',
                                        marker=dict(
                                            size=5,
                                            color=X_tsne[:, 0],
                                            colorscale='Viridis',
                                            opacity=0.8
                                        )
                                    )
                                ])
                                fig_tsne.update_layout(
                                    title='VisualizaciÃ³n t-SNE 3D',
                                    scene=dict(
                                        xaxis_title='t-SNE 1',
                                        yaxis_title='t-SNE 2',
                                        zaxis_title='t-SNE 3'
                                    )
                                )
                                st.plotly_chart(fig_tsne)
                            with st.expander("ðŸ§  InterpretaciÃ³n de t-SNE"):
                                st.markdown("""
                                    ### Objetivo: VisualizaciÃ³n de datos de alta dimensiÃ³n

                                    **CaracterÃ­sticas:**
                                    - Preserva estructura local de los datos
                                    - No lineal: Captura relaciones complejas
                                    - Enfocado en visualizaciÃ³n

                                    **ParÃ¡metros Importantes:**
                                    - **Perplexidad:** Balance entre estructura local y global
                                        - Rango tÃ­pico: 5-50
                                        - Afecta la distribuciÃ³n de puntos
                                    - **NÃºmero de iteraciones:** Afecta la calidad del resultado
                                    
                                    **InterpretaciÃ³n PrÃ¡ctica:**
                                    - Distancias absolutas no son significativas
                                    - Clusters visibles sugieren grupos naturales
                                    - Ãštil para exploraciÃ³n visual de datos
                                    - Complementa otros mÃ©todos de clustering
                                    """)
                        elif metodo == "UMAP":
                            try:
                                reducer = umap.UMAP(n_components=n_components, random_state=42)
                                X_umap = reducer.fit_transform(X_scaled)

                                if n_components == 2:
                                    fig_umap = px.scatter(
                                        x=X_umap[:, 0],
                                        y=X_umap[:, 1],
                                        title='VisualizaciÃ³n UMAP',
                                        labels={'x': 'UMAP 1', 'y': 'UMAP 2'}
                                    )
                                    st.plotly_chart(fig_umap)
                                else:
                                    fig_umap = go.Figure(data=[
                                        go.Scatter3d(
                                            x=X_umap[:, 0],
                                            y=X_umap[:, 1],
                                            z=X_umap[:, 2],
                                            mode='markers',
                                            marker=dict(
                                                size=5,
                                                color=X_umap[:, 0],
                                                colorscale='Plasma',
                                                opacity=0.8
                                            )
                                        )
                                    ])
                                    fig_umap.update_layout(
                                        title='VisualizaciÃ³n UMAP 3D',
                                        scene=dict(
                                            xaxis_title='UMAP 1',
                                            yaxis_title='UMAP 2',
                                            zaxis_title='UMAP 3'
                                        )
                                    )
                                    st.plotly_chart(fig_umap)

                            except ImportError:
                                st.error("El paquete UMAP no estÃ¡ instalado. InstÃ¡lalo con: pip install umap-learn")
                            with st.expander("ðŸ§  InterpretaciÃ³n de UMAP"):
                                st.markdown("""
                                    ### Objetivo: ReducciÃ³n de dimensionalidad y visualizaciÃ³n

                                    **CaracterÃ­sticas:**
                                    - Preserva estructura global y local
                                    - MÃ¡s rÃ¡pido que t-SNE
                                    - Mejor preservaciÃ³n de estructura global

                                    **ParÃ¡metros Clave:**
                                    - **n_neighbors:** Balance entre estructura local/global
                                    - **min_dist:** Controla la compactaciÃ³n de puntos
                                    - **n_components:** Dimensiones de salida

                                    **InterpretaciÃ³n PrÃ¡ctica:**
                                    - MÃ¡s escalable que t-SNE
                                    - Mejor para grandes conjuntos de datos
                                    - Preserva mÃ¡s informaciÃ³n topolÃ³gica
                                    - Ãštil para visualizaciÃ³n y clustering
                                    """)
                    except Exception as e:
                        st.error(f"Error en {metodo}: {e}")

        # SecciÃ³n de ExplicaciÃ³n de ParÃ¡metros
        st.write("---")
        st.write("### ExplicaciÃ³n de ParÃ¡metros")

        # Verificar la clave API de Gemini
        has_api_key = 'gemini_api_key' in st.session_state and st.session_state.gemini_api_key

        if not has_api_key:
            st.warning("Configure su API key de Gemini en la secciÃ³n superior izquierda para usar la explicaciÃ³n automÃ¡tica de los parÃ¡metros.")

        # Inicializar las explicaciones en el estado de sesiÃ³n si no existen
        if 'model_explanations' not in st.session_state:
            st.session_state.model_explanations = {}

        # Crear un botÃ³n para generar la explicaciÃ³n
        explain_button = st.button(
            "Explicar ParÃ¡metros",
            disabled=not has_api_key,
            key=f"explain_params"
        )

        # Mostrar las explicaciones existentes
        if metodos:
            for method in metodos:
                if method in st.session_state.model_explanations:
                    st.markdown(f"### ExplicaciÃ³n de {method}")
                    st.markdown(st.session_state.model_explanations[method])

        if explain_button and has_api_key:
            try:
                with st.spinner("Generando explicaciÃ³n..."):
                    # Configurar Gemini
                    genai.configure(api_key=st.session_state.gemini_api_key)
                    model = genai.GenerativeModel('gemini-1.5-flash')

                    # Generar explicaciones para cada mÃ©todo
                    for method in metodos:
                        # Encontrar los parÃ¡metros para este mÃ©todo
                        method_params = [p for p in params_text.split('\n') if method in p or method.lower() in p.lower()]
                        method_params_text = '\n'.join(method_params) if method_params else "No se especificaron parÃ¡metros adicionales."

                        # Preparar el prompt
                        prompt = f"""Por favor, proporciona una explicaciÃ³n detallada de los parÃ¡metros utilizados en el mÃ©todo de {method} para anÃ¡lisis no supervisado. Incluye ejemplos y explica cÃ³mo los diferentes valores de parÃ¡metros afectan el resultado.

                        {method_params_text}
                        """

                        # Generar la explicaciÃ³n
                        response = model.generate_content(prompt)

                        # Guardar la explicaciÃ³n en el estado de sesiÃ³n
                        st.session_state.model_explanations[method] = response.text

                        # Mostrar la explicaciÃ³n
                        st.markdown(f"### ExplicaciÃ³n de {method}")
                        st.markdown(response.text)

            except Exception as e:
                st.error(f"Error al generar la explicaciÃ³n: {str(e)}")

        # BotÃ³n para guardar resultados
        if st.button("Guardar Resultados"):
            try:
                # Crear DataFrame con resultados
                resultados_df = pd.DataFrame({
                    'MÃ©todo': metodos,
                    'Variables': [', '.join(feature_cols)] * len(metodos),
                    'Silhouette Score': [global_silhouette] if global_silhouette is not None else [None],
                    'Calinski-Harabasz Score': [global_calinski] if global_calinski is not None else [None],
                    'Davies-Bouldin Score': [global_davies] if global_davies is not None else [None]
                })

                # Guardar CSV
                resultados_df.to_csv('unsupervised_results.csv', index=False)
                st.success("Resultados guardados en 'unsupervised_results.csv'")

            except Exception as e:
                st.error(f"Error al guardar resultados: {e}")